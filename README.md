# Modelo de Linguagem - Auto Atenção

Esse projeto foi feito para a disciplina de mestrado da Unicamp IA024 - Redes Neurais Profundas para Processamento de Linguagem Natural.

O objetivo deste projeto foi treinar uma rede neural com auto-atenção para prever a próxima palavra de um texto, dada as palavras anteriores como entrada.

![download](https://github.com/user-attachments/assets/2f236c1c-8d60-443f-b96e-1dc05d56fd38)


A base de dados utilizada foram textos retirados de livros do Machado de Assis (https://github.com/ethelbeluzzi/projetomachado)

As instruções foram:

- É necessário fazer duas implementações da camada de auto-atenção: uma usando laços (ineficiente, mas fácil de entender) e outra matricial (eficiente mas difícil de entender).
- Fazer um assert para garantir que o resultado das duas implementações é exatamente igual.
- No treinamento, usar apenas a implementação matricial.
